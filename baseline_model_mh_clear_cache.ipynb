{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Micah Holness\n",
    "10/13/2024\n",
    "CSC 8850\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.io.wavfile as sw\n",
    "import scipy.signal as ss\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import .wav data & labels\n",
    "img_root = \"/Users/mholness1/Desktop/CSC 8850 Project/techcabal-ewe-audio-translation-challenge20240903-4068-o1ckqz/TechCabal Ewe Audio Files-20241014T013956Z-002/TechCabal Ewe Audio Files/\"\n",
    "csv_data = pd.read_csv('./Train.csv', sep=',')\n",
    "print(csv_data.keys())\n",
    "images = csv_data['audio_filepath']\n",
    "labels = csv_data['class']\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "print(unique_labels)\n",
    "labels_class = np.arange(0,len(np.unique(labels)))\n",
    "print(labels_class)\n",
    "\n",
    "# create class to load data into DataLoader (and preprocess)\n",
    "freq_spectrum = []\n",
    "freq_labels = []\n",
    "for widx, wv in enumerate(images):\n",
    "    fpath = os.path.join(img_root, wv)\n",
    "    if os.path.isfile(fpath) == 1:\n",
    "        fs, data = sw.read(fpath)\n",
    "        class_tmp = labels[widx]\n",
    "        cidx = np.where(class_tmp == unique_labels)[0]\n",
    "        freq_labels.append(cidx)\n",
    "        # convert time domain to frequency domain\n",
    "        if len(data.shape) > 1:\n",
    "            data = data[:,0]\n",
    "        # transforming the 1-D time-series into a frequency spectrum\n",
    "        fft = np.fft.fft(data)\n",
    "        fft_centered = np.fft.fftshift(fft)\n",
    "        fft_magn = np.log10(np.abs(fft_centered)**2)\n",
    "        # print(fft_magn.shape)\n",
    "        fft_magn_dwn = ss.resample(fft_magn, 51744, axis=0).astype(np.float32)\n",
    "        freq_spectrum.append(fft_magn_dwn.T)\n",
    "\n",
    "freq_spectrum_arr = np.stack(freq_spectrum, axis=0)\n",
    "freq_labels_arr = np.array(freq_labels)\n",
    "\n",
    "imgs_length = [freq.shape[0] for freq in freq_spectrum_arr]\n",
    "print(np.min(imgs_length))      # 51744\n",
    "\n",
    "np.save('training_data.npy', freq_spectrum_arr)\n",
    "np.save('training_labels.npy', freq_labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stat\n",
    "\n",
    "# load the training data arrays\n",
    "training_data = np.load('./training_data.npy', allow_pickle=True)\n",
    "training_labels = np.load('./training_labels.npy', allow_pickle=True)\n",
    "\n",
    "print(training_data.shape)      # 799, 51744\n",
    "\n",
    "# visualize data (means)\n",
    "fig = plt.figure()\n",
    "plt.scatter(np.arange(0,training_data.shape[0]), np.mean(training_data, axis=1))\n",
    "\n",
    "# Perform the Shapiro-Wilk test ---> test for normality\n",
    "statistic, p_value = stat.shapiro(np.mean(training_data, axis=1))\n",
    "print(\"Shapiro-Wilk Statistic:\", statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# note: according to Shapiro-Wilk test, data is significantly skewed\n",
    "# thus, when data is non-normal/non-Gaussian, normalization is preferred [for unknown distribution]\n",
    "# normalize data range = 0 -> 1\n",
    "training_data = (training_data - np.min(training_data)) / (np.max(training_data) - np.min(training_data))\n",
    "print(np.min(training_data), np.max(training_data))\n",
    "\n",
    "# one-hot encode labels\n",
    "training_labels_tmp = torch.tensor(training_labels.flatten())\n",
    "training_labels = nn.functional.one_hot(training_labels_tmp)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.imgs = training_data\n",
    "        self.labels = training_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "dataset = MyDataset()\n",
    "dataloader_train = DataLoader(dataset,\n",
    "                              batch_size=64,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0,\n",
    "                              pin_memory=False)\n",
    "\n",
    "print(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your CNN model\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3),\n",
    "            nn.Conv1d(in_channels=32, out_channels=75, kernel_size=3),\n",
    "            nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=75, out_channels=100, kernel_size=3),\n",
    "            nn.Conv1d(in_channels=100, out_channels=75, kernel_size=3),\n",
    "            nn.Conv1d(in_channels=75, out_channels=32, kernel_size=3),\n",
    "            nn.Conv1d(in_channels=32, out_channels=1, kernel_size=3),\n",
    "            nn.MaxPool1d(kernel_size = 2, stride = 2),\n",
    "            nn.AdaptiveAvgPool1d(100)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(100, 225),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(225, 475),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(475, 210),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(210, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, n_classes)\n",
    "        )     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, axis=0)\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "# set weight initialization\n",
    "def init_weights(model):\n",
    "    \"\"\"Set Conv weights to be He initialization (Kaiming uniform distr) for all Convs\n",
    "        bound = gain x sqrt(3 / fan_mode), \n",
    "        where final tensor has bounds (-bound, +bound) from a uniform distri.\n",
    "        gain (multiplicative factor adjusting weights prior to feeding into neurons), that\n",
    "        has influence on the weight magnitudes, by preserving weight magnitudes in backwards pass (fan_out: n = number of inputs to node)\n",
    "        **** He = recommended for use with ReLU ****\n",
    "    \n",
    "        Linear layer weights are truncated normal distribution\n",
    "        mean = 0, std = 1, with all values within bounds a <= u <= b\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv1d)):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_out', nonlinearity='relu') \n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "            nn.init.trunc_normal_(m.weight, mean=0.0, std=1.0, a=-2.0, b=2.0, generator=None)\n",
    "    \n",
    "model = MyCNN(n_classes=7)\n",
    "print(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for didx, data in enumerate(dataloader_train):\n",
    "    print(type(data))\n",
    "    img, label = data\n",
    "    print(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, loss, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dataloader_train):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        outputs = outputs.float()\n",
    "        labels = labels.float()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # assess accuracy\n",
    "        softmax_func = nn.Softmax(dim=1)\n",
    "        softmax = softmax_func(outputs)\n",
    "        acc_vector = torch.stack([(torch.argmax(softmax[iidx])==torch.argmax(labels[iidx])) for iidx in np.arange(0,labels.shape[0])], dim=0).flatten()\n",
    "        accuracy = torch.sum(acc_vector) / labels.shape[0]\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader_train)}], \\\n",
    "              Loss: {loss.item():.4f}, Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
